{
  "title": "Why are you still writing performance reviews?",
  "description": "Your direct reports should be doing a lot of this work for you.",
  "date": "2024-11-28",
  "url": "why-are-you-still-writing-performance-reviews/",
  "type": "post",
  "tags": ["mgmt"],
  "draft": false 
}

This is not a post about using `<your favorite ai tooling>` to write your performance reviews. Don't do that. This is a post about how to leverage your team to do a lot of your performance review work on your behalf. This works best on growth oriented teams that cultivate a high degree of psychological safety ([I have thoughts about achieving this](/a-truth-framework-for-engineering-1-on-1-meetings/)). It will also work better if your leadership style tends toward _servant_ or _transformational._ 

# Prerequisites

In addition to having a growth oriented team and an appropriate leadership style, this performance review technique fits most comfortably within an organization that is large enough to have career levels and a standardized performance assessment framework of some kind. You can make this work with a small team, but it works best for teams of five or more. 

# Peer Evaluations

**Every member of your team must assess every member of your team**, according to their career level and the organization's performance assessment criteria. 

You read that correctly. You are not asking your "Tech Lead" or most senior team member to do reviews for you. You _are_ asking everyone to self-evaluate (but you're likely doing that anyway), and you _are_ asking for every team member's formal assessment of everyone else on the team. 

What? How? Won't that take forever? Why would anyone want to do this? All good questions, we'll get to these. First, a quick example to give us some context and offer a platform for deeper discussion.

## Team Bluth

I manage the Bluth team at Sudden Valley Technologies. Its members are:

 - Lucille (L5)
 - Michael (L3)
 - Gob (L4)
 - Lindsay (L3)
 - George Michael (L2)
 - Maeby (L2)

## Sudden Valley Technology

Sudden Valley uses generative AI to automate the creation of positive reviews for your salad dressing or real estate products. It is not a real company.

The Sudden Valley engineering ladder has seven levels, and there's a document available to all employees detailing skills expectations per level. 

Sudden Valley has organization-wide annual reviews and their performance metrics are:
 - Technical Skill (unique to role)
 - Communication/Collaboration
 - Leadership/Mentoring
 - Innovation/Agility

Performance metrics are regularly assessed according to a 1-5 scale where (1) represents _must improve_, (3) represents _meeting expectations_, and (5) represents _consistently exceeding expectations_.

Regular performance reviews consists of:
 - Self assessment (mandatory)
 - Lead assessment (mandatory)
 - Peer assessment (optional, strongly encouraged for non-3 ratings)

Look familiar?

# Mutually Beneficial

Getting a complete peer evaluation matrix is not delegating your performance review work; it is doing your performance review work _better_.

First of all, a significant portion of this effort is required in most organizations anyway. In our example, Lucille is going to be required to self assess, and she's almost certainly going to provide some degree of assistance on other members of the team. The same is true to some degree for everyone on the team. Is it _more work_? Yes, strictly speaking - it is more work. Nonetheless, it's easy to show that there are benefits to the work, and I think on a team with established trust and communication models the value proposition is clear.

## Team Benefit

When I ask my team to complete a full peer evaluation matrix (Google Sheets/Forms work well), it is important to show how it benefits the team and the individual members. First, make it as simple and quick as possible. The assessment looks something like:

 - Lucille:
   - Technical Skill (1-5)
   - Communication/Collaboration (1-5)
   - Leadership/Mentoring (1-5)
   - Innovation/Agility (1-5)
   - Performance Review Category on which to focus: (**One** of the categories)
 - Michael:
   - Technical Skill (1-5)
   - etc.

We are not requiring examples or doing SBI to support these assessments. We'll get there, but that's not part of the basic peer evaluation effort. That's follow up discussion, and part of the work that you are responsible for collecting. 

### Learn The Rating System

What we are requiring is familiarity with the career leveling document and the performance review categories. Since every member of the team is reviewing every other member of the team, they must understand the specifics of performance review for each career level. Mistakes will be made, and that's ok - remember: follow up discussion. Your team must know that the expectation is to have familiarity with the levels and performance review categories, but that follow up discussion will serve to assist with re-calibrating as necessary.

This familiarity is useful for all the members of the team. I would argue that within SWE/SRE circles we don't place enough importance upon familiarity with levels/performance categories. Have you ever heard about software engineers complaining that they don't know what would be necessary to get a promotion or a good rating? Peer evaluations make that issue go away. They know what's necessary for everyone on the team to get a good rating/promo, it's precisely the exercise. 

### Broaden the review perspective

Armed with career level and performance review category familiarity, we're asking for a simple numeric rating for each category for each team member. We're also asking for an opinionated assertion on which performance review category is most worth improving upon for each team member. I like to make it clear that context matters for the improvement category, but the context is for follow up discussion. For consideration: Maeby may get a (2) in Leadership/Mentoring and a (4) in Technical Skill. Lindsay may suggest that Maeby focus on Leadership/Mentoring to flatten her ratings and seek a promotion; at the same time Lucille may recommend that Maeby lean into her Technical Skill spike and focus her efforts on really solving technical challenges because the team workload is currently high and they need Maeby's help with technical work more than leadership/mentoring right now. Who's right? It's difficult to say, but having both perspectives in mind for a performance review discussion is a great thing. 

### Get Your Gap Analysis For Free

Follow up discussions between lead and report should detail the results of each team member's assessment. There are great 1:1 conversation starters lurking throughout this peer evaluation matrix. 

**Calibration** - Sometimes an individual will rate generally too high or too low. Talk about this and detail how ratings tend to work within the organization. There will always be calibration issues, but this effort should help to minimize them and generally make performance review discussions easier.

**Peer vs Group** - Does an individual's peer assessment drift from the groups assessment of that same peer? In places where the group assessment differs from an individual assessment, can we figure out why? Is it a calibration issue? Was it perspective-specific (awareness of particular projects/situations)?

**Self vs Group** - Compare the self assessment to the group assessment. How do they match? How do they differ? What should you work on? Can you see why things are different or why they're the same? As a leader do you agree more with the self or the group assessment, why?

**Next Steps** - What do you want to work on next? How does your personal development plan map to your suggested focus areas? 

I prefer to have three follow up 1:1 discussions about peer evaluations:
 1. **Review peer evaluations** - sync and calibrate, this is where SBI type references make sense to share and discuss. Ideally, you can do this with all team members before discussing any personal evaluations.
 2. **Review personal evalution** - everyone is focusing on this, but forcing the team to discuss all their _other_ reviews with you first will fully prepare you for an in depth discussion about each individual (now armed with an understanding of how all their peers rated them and why).
 3. **Meta-assessment of the peer evaluation exercise** - likes, dislikes, desire to repeat (at what cadence), suggestions to improve, wishes to cease and desist, whatever. Usually this doesn't take very long.

## Lead Benefit

A full peer evaluation effort provides a spectacular platform for discussing performance and gathering data on each member of the team. As part of this exercise, a team leader gets formal output covering:
 - peer feedback and ratings
 - discussions about ratings
 - insights about projects and activities that you might not have witnessed
 - calibration of your assessment
 - clear cause for development plans and promo/rating discussions

### Writing The Performance Review

Your follow up discussions often contain most of the details for your performance review. The first follow up discussion (asking team members to dive into their assessments of everyone else), is effectively your brainstorming session for every team member's performance review. Don't agree? Have questions? Want examples for things? **Ask!** This is your chance to ensure that everyone on the team endorses (or at least is understands) your assessments. By having these discussions and asking for feedback about all the other members of the team, everyone realizes how invested you are in the accuracy of your performance reviews. This creates more trust and more comfort in the personal evaluation, because the team knows the questions you're asking and the discussions that are involved. When there is a low rating it's usually less of a surprise, and it feels more like you're delivering the assessment of the group. I find that this keeps team members from getting immediately defensive and instead you can talk through the assessment. There's clarity in the fact that you've done your homework and it's not simply a personal attack. This is grounded in the desire for improvement (or recognition) of the team and the team member. 

# Achieve More By Doing Less

This peer evaluation exercise flips the table on writing a bunch of performance reviews. Instead, ask for very simple performance feedback from as many people as possible, then discuss the specifics with everyone to tease out details. Take notes: there you have your performance reviews. You would likely have these discussions anyway, and making a formal process out of it offers team members a peak behind the curtain. It also places some accountability on them for figuring out how to uplevel the team, while showing the value that you put on perspective gathering in performance reviews.

Yes, you ultimately still need to write your performance reviews. Apologies for the clickbait title.

## Anonymous vs Public

I mentioned the importance of the team dynamic and leadership styles at the beginning of this, and I want to close with some notes to that effect.

On the wrong team, I think this exercise could fan the fires of competition and combative behaviors. If your knee-jerk reaction to having team members formally review each other is that you'd have a bunch of people telling you: "I'm awesome and everyone else sucks," then this isn't the exercise for your team. And maybe you should change teams or deal with changing that dynamic (different topic).

In terms of sharing these assessments, I think the sweet spot is that you know who said what, and you share the results only in anonymous aggregate. So Lucille knows (or probably remembers) giving Maeby a (2) for Leadership/Mentoring, but Maeby just sees that three people gave her a (2) and three people gave her a (3) rating for some particular performance review category. There's definite value in the manager/lead knowing what each team member says, and it helps tremendously in assessment calibration. I think there's _some_ benefit to having completely public data, but I think that only sharing the aggregate data makes team members more comfortable with being honest - and that's a bigger win.

# Feedback!?

What do you think? Craziness? Perfection? Meh? 

I've done similar exercises with a handful of teams and had overwhelmingly positive feedback. I've had a couple teams that were too small, and one team where the prospects for this exercise didn't feel positive. I'd love to discuss more; contact info is buried in the [about](/about/) page.

Would this work for your team? What's stopping you?
